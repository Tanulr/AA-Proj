import networkx as nx
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import random as rd

# created a directed graph
graph=nx.gnp_random_graph(25,0.6,directed=True)

#draw a graph
nx.draw(graph,with_labels=True,font_color='red',font_size=10,node_color='yellow')

#plot a graph

plt.show()

#number of nodes for graph
count=graph.number_of_nodes()
#graph neighbours of a node 1
#print(list(graph.neighbors(1)))

#Page Rank Algorithm-Calculating random walk score
rank_dict={}
x=rd.randint(0,25)
for j in range(0,25):
  rank_dict[j]=0
rank_dict[x]=rank_dict[x]+1
for i in range(600000):
  list_n=list(graph.neighbors(x))
  if(len(list_n)==0):
    x=rd.randint(0,25)
    rank_dict[x]=rank_dict[x]+1
  else:
    x=rd.choice(list_n)
    rank_dict[x]=rank_dict[x]+1
  
print("Random Walk Score Updated")

#normalising values
for j in range(0,25):
  rank_dict[j]=rank_dict[j]/600000

#########################################

#Page rank by networkx library
pagerank=nx.pagerank(graph)

#sorting both dictionaries based on items
pagerank_sorted=sorted(pagerank.items(),key=lambda v:(v[1],v[0]),reverse=True)

#sorting the rank_dict based on values
rank_dict_sorted=sorted(rank_dict.items(),key=lambda v:(v[1],v[0]),reverse=True)

###############################################

def pagerank(G, alpha=0.85, personalization=None,
			max_iter=100, tol=1.0e-6, nstart=None, weight='weight',
			dangling=None):

	if len(G) == 0:
		return {}

	if not G.is_directed():
		D = G.to_directed()
	else:
		D = G

	# Create a copy in (right) stochastic form
	W = nx.stochastic_graph(D, weight=weight)
	N = W.number_of_nodes()

	# Choose fixed starting vector if not given
	if nstart is None:
		x = dict.fromkeys(W, 1.0 / N)
	else:
		# Normalized nstart vector
		s = float(sum(nstart.values()))
		x = dict((k, v / s) for k, v in nstart.items())

	if personalization is None:

		# Assign uniform personalization vector if not given
		p = dict.fromkeys(W, 1.0 / N)
	else:
		missing = set(G) - set(personalization)
		if missing:
			raise NetworkXError('Personalization dictionary '
								'must have a value for every node. '
								'Missing nodes %s' % missing)
		s = float(sum(personalization.values()))
		p = dict((k, v / s) for k, v in personalization.items())

	if dangling is None:

		# Use personalization vector if dangling vector not specified
		dangling_weights = p
	else:
		missing = set(G) - set(dangling)
		if missing:
			raise NetworkXError('Dangling node dictionary '
								'must have a value for every node. '
								'Missing nodes %s' % missing)
		s = float(sum(dangling.values()))
		dangling_weights = dict((k, v/s) for k, v in dangling.items())
	dangling_nodes = [n for n in W if W.out_degree(n, weight=weight) == 0.0]

	# power iteration: make up to max_iter iterations
	for _ in range(max_iter):
		xlast = x
		x = dict.fromkeys(xlast.keys(), 0)
		danglesum = alpha * sum(xlast[n] for n in dangling_nodes)
		for n in x:

			# this matrix multiply looks odd because it is
			# doing a left multiply x^T=xlast^T*W
			for nbr in W[n]:
				x[nbr] += alpha * xlast[n] * W[n][nbr][weight]
			x[n] += danglesum * dangling_weights[n] + (1.0 - alpha) * p[n]

		# check convergence, l1 norm
		err = sum([abs(x[n] - xlast[n]) for n in x])
		if err < N*tol:
			return x
	raise NetworkXError('pagerank: power iteration failed to converge '
						'in %d iterations.' % max_iter)

pr=nx.pagerank(graph,0.4)

pr_sorted=sorted(pr.items(),key=lambda v:(v[1],v[0]),reverse=True)

print("The order generated by our implementation algorithm is\n")
for i in rank_dict_sorted:
  print(i[0],end=" ")

print("\n\nThe order generated by networkx library is\n")
for i in pagerank_sorted:
  print(i[0],end=" ")

print("\n\nThe order generated by our implementation algorithm is\n")
for i in pr_sorted:
  print(i[0],end=" ")